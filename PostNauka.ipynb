{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pymongo\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from string import punctuation\n",
    "from natasha import NamesExtractor\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pymongo.MongoClient('mongodb+srv://vera:20003000@cluster0-klabd.mongodb.net/test?retryWrites=true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client['postnauka_ling']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = db.authors\n",
    "articles = db.articles\n",
    "lemmas = db.lemmas\n",
    "tokens = db.tokens\n",
    "tokens_list = db.tokens_list\n",
    "names = db.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'iniForm_1'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors.create_index([('name', pymongo.ASCENDING), ('affiliation', pymongo.ASCENDING), ('articles', pymongo.ASCENDING)], unique=True)\n",
    "articles.create_index([('title', pymongo.ASCENDING), ('lemmas', pymongo.ASCENDING), ('text', pymongo.TEXT), ('tokens', pymongo.ASCENDING)], unique=True)\n",
    "lemmas.create_index([('iniForm', pymongo.ASCENDING)])\n",
    "tokens.create_index([('iniForm', pymongo.ASCENDING)])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_ids = []\n",
    "ling_page_urls = ['https://postnauka.ru/api/v1/posts?page={0}&term=language'.format(n) for n in range(1, 21)]\n",
    "for page_url in ling_page_urls:\n",
    "    posts = requests.get(page_url).json()\n",
    "    page_article_ids = [ post['id'] for post in posts if post['type'] != 'adv']\n",
    "    article_ids.extend(page_article_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_surnames(text):\n",
    "    surnames = []\n",
    "    extractor = NamesExtractor()\n",
    "    matches = extractor(text)\n",
    "    for match in matches:\n",
    "        if (match.fact.last):\n",
    "            surnames.append(match.fact.last)\n",
    "        \n",
    "    return Counter(surnames)\n",
    "\n",
    "# res = extract_surnames('Между тем попытки работы над универсальным семантическим описанием лексики продолжаются. Этим, например, занимается Анна Вежбицкая, которая в 1960-х годах в Варшаве была аспиранткой известного у нас польского семанта Анджея Богуславского. Тогда московская семантическая школа была очень близка к польской. Анна Вежбицкая, продолжая идеи Богуславского, предложила в своей диссертации идею семантических примитивов. Ее суть в том, что существует несколько десятков важнейших понятий, которые имеют словесное выражение во всех языках мира. Это так называемые примитивы, список которых состоит из нескольких десятков единиц. Согласно идеям Анны Вежбицкой, которая привлекала для своих исследований не только польский, английский и русский, но и другие европейские языки, а также данные австралийских языков, языков Южной Азии и другие, в него входят: «я», «хотеть», «хороший», «весь», «представлять себе», «нечто», «знать», «если», «из-за», «люди» и другие. Количество примитивов менялось на разных этапах развития ее теории, но незначительно. Они основа метаязыка Вежбицкой. Из этих примитивов составляются универсальные толкования, которые не всегда легко с ходу понять. Вот, например, как неожиданно выглядит у Анны Вежбицкой толкование такого сложного и культурно-специфичного понятия, как русское слово «пошлость»:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = MorphAnalyzer()\n",
    "punct = punctuation+'«»—…“”*№–'\n",
    "stops = set(stopwords.words('russian'))\n",
    "\n",
    "def normalize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split()]\n",
    "    words = [morph.parse(word)[0].normal_form for word in words if word]\n",
    "    words = [{\"iniForm\": morph.parse(word)[0].normal_form, \"POS\": morph.parse(word)[0].tag.POS} for word in words if word]\n",
    "    return words\n",
    "\n",
    "def tokenize(text):\n",
    "    \n",
    "    words = [word.strip(punct) for word in text.lower().split() if word and word not in stops]\n",
    "    words = [word for word in words if word]\n",
    "\n",
    "    return words\n",
    "\n",
    "\n",
    "\n",
    "def parse_html(html_text):\n",
    "    \n",
    "    soup = BeautifulSoup(html_text, 'lxml')\n",
    "    article_text = soup.getText()\n",
    "    article_tokens = tokenize(article_text)\n",
    "    article_normalized_words = normalize(article_text)\n",
    "    surnames = extract_surnames(article_text)\n",
    "    \n",
    "    return {\n",
    "        'article_text': article_text,\n",
    "        'article_tokens': article_tokens,\n",
    "        'article_normalized_words': article_normalized_words,\n",
    "        'article_surnames': surnames\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vera/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:47: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n"
     ]
    }
   ],
   "source": [
    "article_urls = ['https://postnauka.ru/api/v1/posts/{0}?expand=content'.format(article_id) for article_id in article_ids]\n",
    "\n",
    "for article_url in article_urls:\n",
    "    response = requests.get(article_url).json()\n",
    "    \n",
    "    article_info = parse_html(response['content'])\n",
    "    tokens_ids = []\n",
    "    lemmas_ids = []\n",
    "    \n",
    "    for token in article_info['article_tokens']:\n",
    "        token_id = tokens.insert_one({\n",
    "            'iniForm': token\n",
    "        }).inserted_id\n",
    "        \n",
    "        tokens_ids.append(token_id)\n",
    "    \n",
    "    tokens_list_id = tokens_list.insert_one({\n",
    "        'tokens': tokens_ids\n",
    "    }).inserted_id\n",
    "        \n",
    "        \n",
    "    for lemma in article_info['article_normalized_words']:\n",
    "        lemma_id = lemmas.insert_one({\n",
    "            'iniForm': lemma\n",
    "        }).inserted_id\n",
    "        \n",
    "        lemmas_ids.append(lemma_id)\n",
    "    \n",
    "    article_id = articles.insert_one({ \n",
    "        'title': response['title'],\n",
    "        'text': article_info['article_text'],\n",
    "        'tokens': tokens_list_id,\n",
    "        'lemmas': lemmas_ids,\n",
    "        'surnames': article_info['article_surnames']\n",
    "    }).inserted_id\n",
    "    \n",
    "    article_author = response['authors'][0]\n",
    "    \n",
    "    if not article_author:\n",
    "        continue\n",
    "\n",
    "    author_name = article_author['author_name']\n",
    "    author_affiliations =  article_author['author_description']\n",
    "    \n",
    "    authors_in_base = authors.find({ 'name': author_name })\n",
    "\n",
    "    if authors_in_base.count() == 0:\n",
    "        authors.insert_one({ 'name': author_name, 'affiliations': author_affiliations, 'articles': [article_id] })\n",
    "    else:\n",
    "        author_id=authors_in_base[0][\"_id\"]\n",
    "        authors.find_one_and_update({ '_id': author_id }, {\"$addToSet\": {\"articles\": article_id}})\n",
    "    \n",
    "print('END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surnames = [surnames += Counter(surname) for surname in articles.find({'surnames'})]\n",
    "    \n",
    "surnames"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
